# Development Plan - October 29, 2025

## Project Status

**Current State:**
- 164 tests passing (1 skipped)
- 8 major PRs merged (Week 1-8 deliverables complete)
- Meta-learning, statistical analysis, LLM integration all functional
- Clean working tree on main branch
- Apache 2.0 licensed

**Technical Debt Identified:**
- prototype.py monolithic (800+ lines, multiple concerns)
- Magic numbers hardcoded (0.2, 0.3, 0.5 rates)
- Print-based logging instead of logging module
- No CLI automated testing
- Documentation gaps in README

---

## Immediate Priority (This Week - 6-8 hours total)

### 1. Modular Refactoring ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Problem:** prototype.py contains 800+ lines with mixed concerns (Strategy definitions, EvolutionaryEngine, ComparisonEngine, BaselineStrategyGenerator, CLI logic)

**Solution:**
```
prototype.py ‚Üí main.py (CLI entry point only)

New structure:
src/
‚îú‚îÄ‚îÄ strategy.py         # Strategy, StrategyGenerator, LLMStrategyGenerator
‚îú‚îÄ‚îÄ crucible.py         # FactorizationCrucible
‚îú‚îÄ‚îÄ evolution.py        # EvolutionaryEngine, crossover_strategies, mutate_strategy
‚îú‚îÄ‚îÄ comparison.py       # ComparisonEngine, BaselineStrategyGenerator
‚îú‚îÄ‚îÄ metrics.py          # EvaluationMetrics (currently in prototype.py)
‚îî‚îÄ‚îÄ config.py           # (existing, will be extended)
```

**Implementation Steps:**
1. Create feature branch: `git checkout -b refactor/modular-architecture`
2. Move Strategy, StrategyGenerator ‚Üí src/strategy.py
3. Move FactorizationCrucible ‚Üí src/crucible.py
4. Move EvolutionaryEngine, crossover, mutate ‚Üí src/evolution.py
5. Move ComparisonEngine, BaselineStrategyGenerator ‚Üí src/comparison.py
6. Move EvaluationMetrics ‚Üí src/metrics.py
7. Rename prototype.py ‚Üí main.py, keep only CLI + main()
8. Update all imports in tests/
9. Run full test suite: `pytest tests/ -v` (all 164 must pass)
10. Commit at each logical step

**Benefits:**
- Clear separation of concerns
- Easier testing (no 800-line file to navigate)
- Reduced circular dependency risk
- Foundation for all other improvements

**Estimated Effort:** 3-4 hours

---

### 2. Configuration Management ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Problem:** Magic numbers hardcoded throughout code:
- Elite selection rate: 0.2
- Crossover/mutation/random rates: 0.3, 0.5, 0.2
- Temperature bounds: 0.8, 1.2
- Meta-learning bounds: 0.1, 0.7

**Solution:**
Extend `src/config.py` to centralize all configuration:

```python
@dataclass
class Config:
    # Existing LLM config
    api_key: str
    max_llm_calls: int = 100
    temperature_base: float = 0.8
    temperature_max: float = 1.2
    temperature_scaling_generations: int = 10

    # NEW: Evolution parameters
    elite_selection_rate: float = 0.2
    crossover_rate: float = 0.3
    mutation_rate: float = 0.5
    random_rate: float = 0.2

    # NEW: Meta-learning parameters
    meta_learning_min_rate: float = 0.1
    meta_learning_max_rate: float = 0.7
    adaptation_window: int = 5

    # NEW: Strategy parameter bounds
    power_min: int = 2
    power_max: int = 5
    max_filters: int = 4
    min_hits_min: int = 1
    min_hits_max: int = 6
```

**CLI Integration:**
```bash
# Allow overriding from CLI
--elite-rate 0.3
--crossover-rate 0.4
--mutation-rate 0.4
--meta-learning-min-rate 0.15
```

**Implementation Steps:**
1. Extend Config dataclass in src/config.py
2. Update EvolutionaryEngine.__init__ to accept Config
3. Replace all hardcoded values with config.* references
4. Add CLI arguments for tuneable parameters
5. Update tests to use Config objects
6. Document new flags in README

**Benefits:**
- Experiment tuning without code changes
- Single source of truth for parameters
- Easier to track experiment configurations
- Export config values to metrics JSON

**Estimated Effort:** 1-2 hours

---

### 3. Production Logging ‚≠ê‚≠ê‚≠ê‚≠ê
**Problem:** print() statements scattered throughout code, no log level control, no file output

**Solution:**
```python
# src/config.py
import logging
import os

def setup_logging(level: str = "INFO"):
    """Configure logging for the application."""
    log_level = getattr(logging, level.upper(), logging.INFO)

    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('evolution.log')
        ]
    )

    return logging.getLogger(__name__)

# Usage in main.py
logger = setup_logging(os.getenv('LOG_LEVEL', 'INFO'))
logger.info(f"üéØ Target number: {args.number}")
logger.debug(f"Detailed config: {config}")
```

**Replacement Strategy:**
- `print(f"Generation {i}...")` ‚Üí `logger.info(f"Generation {i}...")`
- `print(f"DEBUG: {details}")` ‚Üí `logger.debug(f"{details}")`
- Keep user-facing console output as print() for simplicity

**Implementation Steps:**
1. Add setup_logging() to src/config.py
2. Add LOG_LEVEL to .env.example
3. Replace print() with logger.* in:
   - main.py (evolution progress)
   - src/evolution.py (generation summaries)
   - src/llm/gemini.py (API call details)
4. Keep print() for final results/summaries (user UX)
5. Add --log-level CLI flag
6. Document logging in README

**Benefits:**
- Detailed debugging without cluttering console
- Log files for post-mortem analysis
- Production-ready observability
- Easy to disable verbose output

**Estimated Effort:** 1-2 hours

---

## High Priority (Next 1-2 Weeks - 5-7 hours total)

### 4. CLI Automated Testing ‚≠ê‚≠ê‚≠ê‚≠ê
**Problem:** No automated tests for argparse combinations, JSON export functionality, or user workflows

**Solution:**
Create `tests/test_cli.py` with subprocess-based smoke tests:

```python
import subprocess
import json
import tempfile
from pathlib import Path

def test_cli_help():
    """Test --help flag."""
    result = subprocess.run(
        ["python", "main.py", "--help"],
        capture_output=True, text=True
    )
    assert result.returncode == 0
    assert "usage:" in result.stdout

def test_cli_basic_run():
    """Test basic evolution run."""
    result = subprocess.run(
        ["python", "main.py", "--generations", "2", "--population", "3"],
        capture_output=True, text=True, timeout=30
    )
    assert result.returncode == 0
    assert "Generation 0" in result.stdout

def test_cli_metrics_export():
    """Test --export-metrics creates valid JSON."""
    with tempfile.TemporaryDirectory() as tmpdir:
        output_path = Path(tmpdir) / "metrics.json"
        result = subprocess.run(
            ["python", "main.py", "--generations", "1", "--population", "2",
             "--export-metrics", str(output_path)],
            capture_output=True, text=True, timeout=30
        )
        assert result.returncode == 0
        assert output_path.exists()

        with open(output_path) as f:
            data = json.load(f)
            assert "target_number" in data
            assert "metrics_history" in data

def test_cli_seed_reproducibility():
    """Test --seed produces identical initial populations."""
    result1 = subprocess.run(
        ["python", "main.py", "--seed", "42", "--generations", "1", "--population", "3"],
        capture_output=True, text=True, timeout=30
    )
    result2 = subprocess.run(
        ["python", "main.py", "--seed", "42", "--generations", "1", "--population", "3"],
        capture_output=True, text=True, timeout=30
    )
    # Extract civilization IDs from output (should be identical)
    assert result1.stdout == result2.stdout

def test_cli_invalid_arguments():
    """Test CLI rejects invalid argument combinations."""
    result = subprocess.run(
        ["python", "main.py", "--population", "-5"],
        capture_output=True, text=True
    )
    assert result.returncode != 0
```

**Coverage Target:**
- ‚úÖ --help
- ‚úÖ Basic evolution run
- ‚úÖ --export-metrics
- ‚úÖ --export-comparison
- ‚úÖ --seed reproducibility
- ‚úÖ --meta-learning
- ‚úÖ --llm mode (skip if no API key)
- ‚úÖ Invalid argument handling

**Benefits:**
- Prevents user-facing regressions
- Documents expected CLI behavior
- CI catches breaking changes early

**Estimated Effort:** 2-3 hours

---

### 5. Checkpoint/Resume Functionality ‚≠ê‚≠ê‚≠ê‚≠ê
**Problem:** Long-running experiments (100+ generations) cannot be interrupted and resumed

**Solution:**
Add checkpoint saving and loading:

```python
# src/evolution.py
import pickle
from pathlib import Path

class EvolutionaryEngine:
    def save_checkpoint(self, checkpoint_dir: Path, generation: int):
        """Save current evolution state."""
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        checkpoint_path = checkpoint_dir / f"gen_{generation}.pkl"

        state = {
            'generation': generation,
            'civilizations': self.civilizations,
            'best_fitness_history': self.best_fitness_history,
            'metrics_history': self.metrics_history,
            'config': self.config,
            'random_state': random.getstate()
        }

        with open(checkpoint_path, 'wb') as f:
            pickle.dump(state, f)

        logger.info(f"üíæ Checkpoint saved: {checkpoint_path}")

    @classmethod
    def load_checkpoint(cls, checkpoint_path: Path):
        """Load evolution state from checkpoint."""
        with open(checkpoint_path, 'rb') as f:
            state = pickle.load(f)

        random.setstate(state['random_state'])

        engine = cls(
            target_number=state['config'].target_number,
            # ... other config params
        )
        engine.civilizations = state['civilizations']
        engine.best_fitness_history = state['best_fitness_history']
        engine.metrics_history = state['metrics_history']

        return engine, state['generation']

# main.py CLI integration
parser.add_argument('--checkpoint-dir', type=Path, help='Directory for checkpoints')
parser.add_argument('--resume-from', type=Path, help='Resume from checkpoint file')
parser.add_argument('--checkpoint-every', type=int, default=10,
                    help='Save checkpoint every N generations')

# Usage
if args.resume_from:
    engine, start_gen = EvolutionaryEngine.load_checkpoint(args.resume_from)
else:
    engine = EvolutionaryEngine(...)
    start_gen = 0

for gen in range(start_gen, args.generations):
    # ... evolution loop

    if args.checkpoint_dir and (gen + 1) % args.checkpoint_every == 0:
        engine.save_checkpoint(args.checkpoint_dir, gen + 1)
```

**CLI Usage:**
```bash
# Start new run with checkpointing
python main.py --generations 100 --checkpoint-dir ./checkpoints --checkpoint-every 10

# Resume from checkpoint
python main.py --resume-from ./checkpoints/gen_50.pkl --generations 100
```

**Benefits:**
- Long experiments can be interrupted (Ctrl+C)
- Resume after machine restart
- Experiment with different continuation strategies
- Safe from accidental termination

**Estimated Effort:** 3-4 hours

---

### 6. CONTRIBUTING.md Documentation ‚≠ê‚≠ê‚≠ê
**Problem:** No contribution guidelines for external collaborators

**Solution:**
Create CONTRIBUTING.md with:

```markdown
# Contributing to Factorization

## Development Setup

1. Fork and clone the repository
2. Install dependencies: `pip install -r requirements.txt`
3. Set up pre-commit hooks (optional): `pre-commit install`

## Testing

Run all tests: `pytest tests/ -v`
Run with coverage: `pytest tests/ --cov=src --cov-report=html`

## Code Style

- Use ruff for linting: `ruff check .`
- Use ruff for formatting: `ruff format .`
- Follow PEP 8 conventions
- Type hints required for public APIs

## Pull Request Process

1. Create feature branch: `git checkout -b feature/your-feature`
2. Write tests first (TDD)
3. Ensure all tests pass
4. Update documentation (README.md, CLAUDE.md if adding patterns)
5. Commit with conventional commit format:
   - `feat:` new feature
   - `fix:` bug fix
   - `docs:` documentation only
   - `test:` test additions
   - `refactor:` code restructuring
6. Push and create PR
7. Ensure CI passes

## Research Contributions

- Document new heuristics with mathematical justification
- Include baseline comparisons with statistical tests
- Export metrics for reproducibility
- Add experiments to analysis/ notebooks

## Questions?

Open an issue or discussion on GitHub.
```

**Benefits:**
- Lowers barrier to external contributions
- Standardizes development workflow
- Documents expectations clearly

**Estimated Effort:** 30 minutes

---

## Medium Priority (2-4 Weeks)

### 7. Documentation Cleanup
**Tasks:**
- Remove "[Add license information]" from README (LICENSE exists)
- Remove "[Add contribution guidelines]" (after Task 6)
- Extract long "Session Handover" section to docs/history.md
- Restructure README: Setup ‚Üí Usage ‚Üí Features ‚Üí Development
- Add theoretical background section explaining GNFS context

**Estimated Effort:** 1-2 hours

---

### 8. Parallel Evaluation
**Trigger:** When experiments exceed 30 minutes runtime

**Solution:**
```python
from concurrent.futures import ProcessPoolExecutor

def evaluate_population_parallel(crucible, civilizations, duration, max_workers=4):
    """Evaluate strategies in parallel."""
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = {
            civ_id: executor.submit(crucible.evaluate_strategy_detailed,
                                   strategy, duration)
            for civ_id, data in civilizations.items()
            for strategy in [data["strategy"]]
        }

        results = {}
        for civ_id, future in futures.items():
            results[civ_id] = future.result()

        return results
```

**CLI:**
```bash
--parallel-workers 8  # Use 8 cores for evaluation
```

**Estimated Effort:** 2-3 hours

---

### 9. Meta-Learning Visualization
**Goal:** Complete PR #18 user experience (like PR #16 completed PR #14)

**Implementation:**
- Add section to `analysis/visualize_metrics.ipynb`
- Plot 1: Operator rate evolution over generations
- Plot 2: Success rate trends per operator
- Plot 3: Fitness improvement correlation with rate changes
- Update README with usage instructions

**Estimated Effort:** 2-3 hours

---

### 10. Multi-LLM Provider Support
**Trigger:** When comparing LLM effectiveness becomes a research goal

**Solution:**
```python
# src/llm/openai.py
class OpenAIProvider(LLMProvider):
    async def propose_mutation(self, ...):
        # OpenAI API implementation

# src/llm/anthropic.py
class AnthropicProvider(LLMProvider):
    async def propose_mutation(self, ...):
        # Claude API implementation

# CLI usage
--llm-provider gemini|openai|anthropic
```

**Estimated Effort:** 4-6 hours (per new provider)

---

## Low Priority / Deferred

### 11. MLflow/W&B Experiment Tracking
**Trigger:** When managing 50+ experiment runs becomes unwieldy
**Why Deferred:** JSON export + git commits sufficient for current scale
**Estimated Effort:** 6-8 hours

---

### 12. Distributed Computing (Ray/Dask)
**Trigger:** When single-machine parallelization insufficient
**Why Deferred:** Current experiments fit on single machine
**Estimated Effort:** 10-15 hours

---

### 13. LLM Code Execution Sandboxing
**Trigger:** IF project evolves to execute arbitrary LLM-generated code
**Status:** VALIDATE FIRST - Current implementation uses structured JSON only
**Action Required:** Review src/llm/gemini.py to confirm no eval()/exec()
**If needed:** multiprocessing + resource.setrlimit + timeout
**Estimated Effort:** 8-10 hours (if actually needed)

---

### 14. Enhanced Fitness Metrics
**Goal:** Move beyond "candidate count" to actual smoothness analysis
**Features:**
- Prime factorization depth
- B-smooth validation
- False positive rate
- Diversity metrics (unique residue classes)

**Estimated Effort:** 6-8 hours

---

## Implementation Timeline

| Timeframe | Tasks | Total Hours |
|-----------|-------|-------------|
| **This Week** | Tasks 1-3 (Refactor, Config, Logging) | 6-8h |
| **Week 2** | Tasks 4-6 (CLI Tests, Checkpoints, CONTRIBUTING) | 5-7h |
| **Week 3** | Tasks 7-9 (Docs, Parallel, Meta-viz) | 5-7h |
| **Week 4+** | Task 10+ (Multi-LLM, etc.) | TBD |

---

## Success Metrics

**Immediate Phase (Tasks 1-3):**
- ‚úÖ prototype.py < 200 lines (only CLI)
- ‚úÖ All tests passing (164+)
- ‚úÖ No magic numbers in code
- ‚úÖ Logging to file enabled

**High Priority Phase (Tasks 4-6):**
- ‚úÖ CLI tests covering major workflows
- ‚úÖ Checkpoint/resume working for 100+ generation runs
- ‚úÖ CONTRIBUTING.md complete

**Medium Phase:**
- ‚úÖ Parallel evaluation reduces runtime by 3-4x
- ‚úÖ Meta-learning visualization complete
- ‚úÖ Multi-LLM support enables comparison research

---

## Risks & Mitigations

**Risk 1: Refactoring breaks existing code**
- Mitigation: Run full test suite after each move
- Mitigation: Commit at logical steps for easy rollback

**Risk 2: Configuration complexity increases**
- Mitigation: Keep sensible defaults
- Mitigation: Document all new flags in README

**Risk 3: Checkpoint overhead slows evolution**
- Mitigation: Make checkpointing optional (--checkpoint-dir)
- Mitigation: Configurable frequency (--checkpoint-every)

---

## Next Immediate Step

**Start with Task 1: Modular Refactoring**

```bash
# Create feature branch BEFORE any work (CLAUDE.md mandatory)
git checkout -b refactor/modular-architecture

# Follow TDD workflow:
# 1. Ensure all tests pass before refactoring
# 2. Move code incrementally
# 3. Run tests after each move
# 4. Commit at logical milestones

# Expected outcome:
# - prototype.py ‚Üí main.py (~150 lines)
# - New src/ modules created
# - All 164 tests still passing
# - Foundation for Tasks 2-3
```

This refactoring is the foundation for ALL other improvements - configuration management, logging, testing all become easier with modular architecture.
