# AI Civilization Research Plan - November 2, 2025

## Executive Summary

**Research Goal**: Test whether specialized AI agents collaborating produce emergent capabilities beyond what individual agents achieve independently.

**Testbed**: Factorization strategy evolution (domain is irrelevant - the focus is cognitive emergence)

**Approach**: C3 (Hybrid) → C4 (Alternative Models)
- **Phase 1 (C3)**: Validate collaboration architecture, then test LLM enhancement
- **Phase 2 (C4)**: Explore novel collaboration models using validated foundation

**Timeline**: 4-6 weeks total
**Budget**: ~$5-10
**Expected Outcomes**: 2-4 publications on multi-agent cognitive systems

---

## Strategic Vision: Why This Path

### The Core Research Question

**Can specialized AI agents collaborating produce emergent capabilities beyond individual agents?**

This is NOT about:
- ❌ Optimizing factorization algorithms
- ❌ Finding the best evolutionary operators
- ❌ Minimizing API costs or project risk

This IS about:
- ✅ Testing cognitive specialization (SearchSpecialist vs EvaluationSpecialist)
- ✅ Understanding feedback loops in multi-agent systems
- ✅ Discovering whether LLM reasoning enhances collaboration
- ✅ Exploring what collaboration structures enable collective intelligence

### Why C3 → C4 (Not Meta-Learning or Methodology)

**Meta-Learning (Option A)**:
- Optimizes single-agent performance (no collaboration tested)
- Traditional genetic algorithm tuning (not AI civilization research)
- Useful as control group, but not the research question

**Methodology (Option B)**:
- Infrastructure work (necessary but not research itself)
- Tool sharpening, not hypothesis testing

**Prometheus C3 + C4**:
- Tests core AI civilization hypotheses
- Produces findings regardless of outcome (negative results publishable)
- Progressive validation: simple → complex collaboration models
- Generalizable beyond factorization domain

### Why Phase 1 "Failed" (It Didn't)

**Investigation Finding**: Prometheus collaborative mode underperformed by 11%

**Root Causes**:
1. No feedback integration (agents.py:157) - feedback collected but not used
2. No selection pressure (experiment.py:141-191) - no elite selection
3. Message overhead (communication.py) - 1200 messages with zero benefit

**Critical Insight**: This is NOT a bug, it's **incomplete implementation**
- Phase 1 MVP: Infrastructure validation ✅ (458 tests passing)
- Phase 2: Learning mechanisms (feedback loops, selection) ← We are here
- Hypothesis H1: Collaboration > Independence ← Cannot test until Phase 2

**Conclusion**: Phase 1 validated the infrastructure. Now we implement the actual collaboration.

---

## Phase 1 (C3): Hybrid Approach - Weeks 1-3

### Strategic Rationale

**Progressive validation with two decision points**:
1. Week 1 end: Does rule-based collaboration architecture work?
2. Week 3 end: Does LLM reasoning enhance collaboration?

**Risk mitigation**:
- Rule-based first ($0 cost) validates architecture before LLM investment
- If C1 fails: Fix collaboration mechanism (architecture broken)
- If C1 works: Proceed to C2 knowing foundation is solid

**Research value**:
- **Two hypotheses tested**: H1a (collaboration), H1b (LLM enhancement)
- **Two publications**: Mechanism paper (C1), Enhancement paper (C2)
- **Negative results publishable**: "LLMs don't improve collaboration" is a finding

---

### Week 1: C1 - Rule-Based Collaborative Evolution

#### Research Question (H1a)
**Does cognitive specialization with feedback integration produce emergence?**

**Hypothesis**: SearchSpecialist with EvaluationSpecialist feedback > SearchSpecialist alone

**Success Criteria**:
- Emergence factor > 1.1 (collaborative beats best baseline by 10%+)
- p-value < 0.05 (statistically significant)
- Cohen's d ≥ 0.5 (medium effect size)

#### Implementation Tasks

**1. Feedback Integration (agents.py:157) - 4 hours**

**Current State**:
```python
def process_request(self, message: Message) -> Message:
    # Phase 1 MVP: Generate random strategy for infrastructure validation
    # LLM-guided generation with feedback is planned for Phase 2
    strategy = self.generator.generate_random_strategy()
    return Message(...)
```

**New Implementation**:
```python
def process_request(self, message: Message) -> Message:
    """
    Generate strategy using EvaluationSpecialist feedback.

    Feedback integration: Extract trends from conversation history,
    use to guide rule-based mutations.
    """
    feedback_context = self._extract_feedback_context()

    if feedback_context and len(feedback_context) > 0:
        # Use feedback to guide mutations
        strategy = self._generate_feedback_guided_strategy(feedback_context)
    else:
        # First generation: no feedback yet
        strategy = self.generator.generate_random_strategy()

    return Message(
        sender_id=self.agent_id,
        recipient_id=message.sender_id,
        message_type="strategy_response",
        payload={"strategy": strategy.to_dict()},
        timestamp=time.time()
    )

def _generate_feedback_guided_strategy(self, feedback_context: List[Dict]) -> Strategy:
    """
    Generate strategy using rule-based mutations guided by feedback.

    Feedback analysis:
    - If "slow evaluation" → reduce power, increase filters
    - If "low smoothness" → adjust smoothness_bound, min_hits
    - If "few candidates" → increase power, reduce filters
    - If "good performance" → small refinement mutations
    """
    # Get last strategy and its fitness
    last_feedback = feedback_context[-1]
    base_strategy = Strategy.from_dict(last_feedback["strategy"])
    fitness = last_feedback["fitness"]

    # Parse feedback text for guidance signals
    feedback_text = last_feedback.get("feedback_text", "").lower()

    # Decision tree based on feedback
    if "slow" in feedback_text or "timeout" in feedback_text:
        # Reduce computational cost
        mutated = self._mutate_for_speed(base_strategy)
    elif "few candidates" in feedback_text or "low fitness" in feedback_text:
        # Increase candidate generation
        mutated = self._mutate_for_coverage(base_strategy)
    elif "low smoothness" in feedback_text or "not smooth" in feedback_text:
        # Improve candidate quality
        mutated = self._mutate_for_quality(base_strategy)
    elif "good" in feedback_text or "high fitness" in feedback_text:
        # Fine-tune successful strategy
        mutated = self._mutate_refinement(base_strategy)
    else:
        # No clear signal: exploratory mutation
        mutated = self.generator.mutate_strategy(base_strategy)

    return mutated

def _mutate_for_speed(self, strategy: Strategy) -> Strategy:
    """Reduce power or increase filters to speed up evaluation."""
    new_strategy = copy.deepcopy(strategy)
    if strategy.power > 2:
        new_strategy.power = strategy.power - 1
    # Add filter to reduce candidates
    if len(strategy.modulus_filters) < 4:
        new_modulus = random.choice([m for m in [7, 11, 13, 17] if m not in [f[0] for f in strategy.modulus_filters]])
        new_residues = [0, random.randint(1, new_modulus-1)]
        new_strategy.modulus_filters.append((new_modulus, new_residues))
    return new_strategy

def _mutate_for_coverage(self, strategy: Strategy) -> Strategy:
    """Increase power or reduce filters to find more candidates."""
    new_strategy = copy.deepcopy(strategy)
    if strategy.power < 5:
        new_strategy.power = strategy.power + 1
    # Remove a filter to increase candidates
    if len(strategy.modulus_filters) > 1:
        new_strategy.modulus_filters.pop()
    return new_strategy

def _mutate_for_quality(self, strategy: Strategy) -> Strategy:
    """Adjust smoothness bounds to improve candidate quality."""
    new_strategy = copy.deepcopy(strategy)
    # Increase smoothness bound (check more primes)
    current_idx = SMALL_PRIMES.index(strategy.smoothness_bound)
    if current_idx < len(SMALL_PRIMES) - 1:
        new_strategy.smoothness_bound = SMALL_PRIMES[current_idx + 1]
    # Increase min hits requirement
    if strategy.min_small_prime_hits < 6:
        new_strategy.min_small_prime_hits = strategy.min_small_prime_hits + 1
    return new_strategy

def _mutate_refinement(self, strategy: Strategy) -> Strategy:
    """Small mutations to refine successful strategy."""
    new_strategy = copy.deepcopy(strategy)
    mutation_choice = random.choice(["filter", "hits", "bound"])

    if mutation_choice == "filter" and strategy.modulus_filters:
        # Modify one filter's residues
        idx = random.randrange(len(strategy.modulus_filters))
        modulus, residues = strategy.modulus_filters[idx]
        # Add or remove one residue
        if len(residues) < 3 and random.random() < 0.5:
            new_res = random.randint(0, modulus-1)
            if new_res not in residues:
                residues.append(new_res)
        elif len(residues) > 1:
            residues.pop()
        new_strategy.modulus_filters[idx] = (modulus, sorted(residues))

    elif mutation_choice == "hits":
        # Small adjustment to min_hits
        delta = random.choice([-1, 1])
        new_strategy.min_small_prime_hits = max(1, min(6, strategy.min_small_prime_hits + delta))

    else:  # "bound"
        # Move smoothness_bound up or down one step
        current_idx = SMALL_PRIMES.index(strategy.smoothness_bound)
        delta = random.choice([-1, 1])
        new_idx = max(0, min(len(SMALL_PRIMES)-1, current_idx + delta))
        new_strategy.smoothness_bound = SMALL_PRIMES[new_idx]

    return new_strategy
```

**Design Rationale**:
- **Structured feedback parsing**: Extract actionable signals from text feedback
- **Domain-informed heuristics**: Use GNFS knowledge (power/filters tradeoff)
- **Progressive refinement**: Explore → Exploit based on feedback
- **Fallback to exploration**: If no clear signal, use random mutation

**Tests to Write** (tests/prometheus/test_feedback_integration.py):
```python
def test_feedback_guided_mutation_slow():
    """SearchSpecialist reduces power when feedback indicates slow evaluation."""

def test_feedback_guided_mutation_quality():
    """SearchSpecialist increases smoothness checks when quality low."""

def test_feedback_guided_mutation_coverage():
    """SearchSpecialist increases power when few candidates found."""

def test_feedback_guided_mutation_refinement():
    """SearchSpecialist makes small changes when performance good."""

def test_no_feedback_uses_random():
    """SearchSpecialist generates random strategy when no feedback available."""
```

---

**2. Selection Pressure (experiment.py:141-191) - 4 hours**

**Current State**:
```python
def run_collaborative_evolution(self, generations: int, population_size: int) -> float:
    """Run collaborative evolution with SearchSpecialist and EvaluationSpecialist."""
    # Each generation: generate fresh random strategies, no inheritance
    for generation in range(generations):
        for i in range(population_size):
            strategy = search_specialist.process_request(...)
            fitness = eval_specialist.process_request(...)
    return best_fitness
```

**New Implementation**:
```python
def run_collaborative_evolution(
    self,
    generations: int,
    population_size: int,
    elite_rate: float = 0.2,
    crossover_rate: float = 0.3,
    mutation_rate: float = 0.5
) -> Tuple[float, Strategy]:
    """
    Run collaborative evolution with elite selection and inheritance.

    Evolutionary loop:
    1. Evaluate population using EvaluationSpecialist
    2. Select top elites (top 20%)
    3. Generate next generation:
       - 30% from crossover of two elites
       - 50% from mutation of one elite (using SearchSpecialist feedback)
       - 20% fresh random strategies
    4. Track best strategy and fitness

    Returns:
        Tuple of (best_fitness, best_strategy)
    """
    # Initialize population
    population: List[Tuple[Strategy, float]] = []

    # Generation 0: random initialization
    for i in range(population_size):
        strategy = self.search_specialist.process_request(
            Message(
                sender_id="orchestrator",
                recipient_id=self.search_specialist.agent_id,
                message_type="strategy_request",
                payload={},
                timestamp=time.time()
            )
        ).payload["strategy"]

        # Evaluate via EvaluationSpecialist
        eval_response = self.eval_specialist.process_request(
            Message(
                sender_id="orchestrator",
                recipient_id=self.eval_specialist.agent_id,
                message_type="evaluation_request",
                payload={"strategy": strategy},
                timestamp=time.time()
            )
        )
        fitness = eval_response.payload["fitness"]

        population.append((Strategy.from_dict(strategy), fitness))

    # Track best across all generations
    best_strategy, best_fitness = max(population, key=lambda x: x[1])

    # Evolution loop
    for generation in range(1, generations):
        # 1. Select elites
        population.sort(key=lambda x: x[1], reverse=True)
        n_elites = max(1, int(population_size * elite_rate))
        elites = population[:n_elites]

        # 2. Generate next generation
        next_generation = []

        # Crossover offspring
        n_crossover = int(population_size * crossover_rate)
        for _ in range(n_crossover):
            parent1, _ = random.choice(elites)
            parent2, _ = random.choice(elites)
            child = self._crossover_strategies(parent1, parent2)
            next_generation.append(child)

        # Mutation offspring (feedback-guided)
        n_mutation = int(population_size * mutation_rate)
        for _ in range(n_mutation):
            parent, parent_fitness = random.choice(elites)

            # Request mutation from SearchSpecialist with feedback context
            mutation_request = Message(
                sender_id="orchestrator",
                recipient_id=self.search_specialist.agent_id,
                message_type="mutation_request",
                payload={
                    "parent_strategy": parent.to_dict(),
                    "parent_fitness": parent_fitness,
                    "generation": generation
                },
                timestamp=time.time()
            )
            mutated = self.search_specialist.process_request(mutation_request)
            next_generation.append(Strategy.from_dict(mutated.payload["strategy"]))

        # Random newcomers (diversity)
        n_random = population_size - len(next_generation)
        for _ in range(n_random):
            random_strategy = self.generator.generate_random_strategy()
            next_generation.append(random_strategy)

        # 3. Evaluate next generation
        new_population = []
        for strategy in next_generation:
            eval_response = self.eval_specialist.process_request(
                Message(
                    sender_id="orchestrator",
                    recipient_id=self.eval_specialist.agent_id,
                    message_type="evaluation_request",
                    payload={"strategy": strategy.to_dict()},
                    timestamp=time.time()
                )
            )
            fitness = eval_response.payload["fitness"]
            new_population.append((strategy, fitness))

        # 4. Update population and track best
        population = new_population
        gen_best_strategy, gen_best_fitness = max(population, key=lambda x: x[1])
        if gen_best_fitness > best_fitness:
            best_fitness = gen_best_fitness
            best_strategy = gen_best_strategy

    return best_fitness, best_strategy

def _crossover_strategies(self, parent1: Strategy, parent2: Strategy) -> Strategy:
    """Uniform crossover of two parent strategies."""
    # Reuse existing crossover_strategies function
    from src.strategy import crossover_strategies
    return crossover_strategies(parent1, parent2)
```

**Design Rationale**:
- **Elite selection**: Top 20% become parents (proven effective in rule-based mode)
- **Three reproduction operators**: Crossover, mutation (feedback-guided), random
- **Feedback integration**: Mutations use SearchSpecialist with parent context
- **Best tracking**: Return best strategy across all generations, not just final

**Tests to Write** (tests/prometheus/test_collaborative_evolution.py):
```python
def test_collaborative_evolution_has_selection():
    """Collaborative mode now uses elite selection like rule-based."""

def test_collaborative_evolution_inheritance():
    """Next generation inherits from elites, not fresh random."""

def test_collaborative_evolution_operators():
    """Correct proportion of crossover/mutation/random offspring."""

def test_collaborative_evolution_feedback_used():
    """SearchSpecialist receives parent context for mutations."""
```

---

**3. Enhanced Feedback Generation (agents.py:171-189) - 2 hours**

**Current State**:
```python
def _generate_feedback(self, strategy: Strategy, metrics: EvaluationMetrics) -> str:
    """Generate basic feedback text."""
    return f"Fitness: {metrics.candidate_count}, smoothness: {avg_smoothness}"
```

**New Implementation**:
```python
def _generate_feedback(self, strategy: Strategy, metrics: EvaluationMetrics) -> str:
    """
    Generate actionable feedback for SearchSpecialist.

    Feedback categories:
    - Performance (fitness relative to baseline)
    - Quality (smoothness scores)
    - Efficiency (timing bottlenecks)
    - Suggestions (what to try next)
    """
    feedback_parts = []

    # 1. Performance assessment
    fitness = metrics.candidate_count
    feedback_parts.append(f"Fitness: {fitness} candidates found")

    if fitness == 0:
        feedback_parts.append("CRITICAL: No candidates found. Strategy too restrictive.")
    elif fitness < 1000:
        feedback_parts.append("LOW fitness. Consider increasing power or reducing filters.")
    elif fitness < 10000:
        feedback_parts.append("MODERATE fitness. Room for improvement.")
    elif fitness < 50000:
        feedback_parts.append("GOOD fitness. Small refinements recommended.")
    else:
        feedback_parts.append("EXCELLENT fitness. Try small variations to explore neighborhood.")

    # 2. Quality assessment
    if metrics.smoothness_scores:
        avg_smoothness = sum(metrics.smoothness_scores) / len(metrics.smoothness_scores)
        feedback_parts.append(f"Average smoothness: {avg_smoothness:.2e}")

        if avg_smoothness > 1e12:
            feedback_parts.append("Smoothness too high (candidates not smooth). Increase smoothness_bound or min_hits.")
        elif avg_smoothness < 1e9:
            feedback_parts.append("Excellent smoothness quality.")

    # 3. Efficiency assessment
    timing = metrics.timing_breakdown
    total_time = sum(timing.values())
    if total_time > 0:
        filter_pct = timing.get("modulus_filtering", 0) / total_time * 100
        smooth_pct = timing.get("smoothness_check", 0) / total_time * 100

        if smooth_pct > 50:
            feedback_parts.append(f"SLOW: {smooth_pct:.1f}% time in smoothness checks. Reduce smoothness_bound.")
        if filter_pct > 30:
            feedback_parts.append(f"Filter overhead: {filter_pct:.1f}%. Consider fewer or simpler filters.")

    # 4. Strategy-specific suggestions
    if strategy.power >= 4 and fitness < 10000:
        feedback_parts.append("High power not yielding results. Try reducing to 2-3.")
    if len(strategy.modulus_filters) == 0 and fitness > 50000:
        feedback_parts.append("No filters with high fitness. Adding filters might improve quality without hurting fitness much.")
    if strategy.min_small_prime_hits >= 5 and fitness < 1000:
        feedback_parts.append("Min hits too high, blocking candidates. Reduce to 2-3.")

    return " | ".join(feedback_parts)
```

**Design Rationale**:
- **Actionable feedback**: Specific suggestions SearchSpecialist can act on
- **Multi-dimensional**: Performance, quality, efficiency
- **Threshold-based**: Clear decision boundaries for rule-based mutations
- **Domain-informed**: Uses GNFS knowledge about power/filter tradeoffs

---

**4. Experiments & Validation - 8 hours**

**Experiment Protocol**:
```bash
# Baseline: Rule-based (control)
for seed in {6000..6009}; do
  python main.py --generations 20 --population 20 --duration 1.0 --seed $seed \
    --export-metrics results/c1_validation/rulebased_${seed}.json
done

# Baseline: Search-only (independent agent)
for seed in {6000..6009}; do
  python main.py --prometheus --prometheus-mode search_only \
    --generations 20 --population 20 --duration 1.0 --seed $seed \
    --export-metrics results/c1_validation/search_only_${seed}.json
done

# Treatment: Collaborative with feedback (C1)
for seed in {6000..6009}; do
  python main.py --prometheus --prometheus-mode collaborative \
    --generations 20 --population 20 --duration 1.0 --seed $seed \
    --export-metrics results/c1_validation/collaborative_c1_${seed}.json
done
```

**Statistical Analysis**:
```python
from src.statistics import StatisticalAnalyzer

analyzer = StatisticalAnalyzer()

# H1a: Collaborative > Search-only (tests cognitive specialization)
result_h1a = analyzer.compare_fitness_distributions(
    evolved_scores=collaborative_c1_fitness,
    baseline_scores=search_only_fitness
)

# Also compare: Collaborative vs Rule-based (tests if collaboration competitive)
result_rulebased = analyzer.compare_fitness_distributions(
    evolved_scores=collaborative_c1_fitness,
    baseline_scores=rulebased_fitness
)
```

**Success Criteria**:
- **H1a confirmed**: p < 0.05 AND d ≥ 0.5 AND emergence_factor > 1.1
- **Proceed to Week 2-3**: If H1a confirmed, LLM enhancement worth trying
- **Fix and retry**: If H1a fails, debug feedback integration before LLM

**Decision Tree**:
```
H1a Result?
├─ SUCCESS (p<0.05, d≥0.5, EF>1.1)
│  └─ Proceed to C2 (LLM enhancement)
│     Publication 1: "Cognitive Specialization in Multi-Agent Evolution"
│
├─ MARGINAL (p<0.1, d≥0.3, EF>1.05)
│  ├─ Analyze: Why marginal? Feedback quality? Selection pressure?
│  ├─ Improve feedback heuristics
│  └─ Re-run with fixes
│
└─ FAILURE (p≥0.1, d<0.3, EF<1.05)
   ├─ Debug: Is feedback being used? Are mutations guided?
   ├─ Check logs: Verify SearchSpecialist processes feedback
   ├─ Fix bugs and re-run
   └─ If still fails: Architecture issue, rethink design
```

---

### Weeks 2-3: C2 - LLM-Guided Collaborative Evolution

#### Research Question (H1b)
**Does LLM reasoning enhance feedback-guided collaboration beyond rule-based?**

**Hypothesis**: LLM-guided mutations with feedback > rule-based mutations with feedback

**Success Criteria**:
- LLM collaborative > C1 collaborative (p < 0.05, d ≥ 0.5)
- Emergence factor (LLM) > emergence factor (C1)
- Cost-effectiveness: improvement justifies ~$5 API cost

#### Implementation Tasks

**1. LLM Prompt Engineering (llm/gemini.py) - 8 hours**

**Enhanced Prompt with Feedback Context**:
```python
def propose_mutation_with_feedback(
    self,
    strategy: Strategy,
    fitness: float,
    feedback_text: str,
    fitness_history: List[float],
    generation: int,
    max_generations: int
) -> LLMResponse:
    """
    Propose mutation using EvaluationSpecialist's feedback.

    New: Include feedback text and context for LLM reasoning.
    """
    # Temperature scaling (exploration → exploitation)
    progress = generation / max_generations
    temperature = self.config.temperature_max - (
        self.config.temperature_max - self.config.temperature_base
    ) * progress

    prompt = f"""You are a SearchSpecialist in an AI civilization working on GNFS factorization.

**Your Role**: Generate novel strategies to find "smooth numbers" (numbers with many small prime factors).

**Current Strategy**:
- Power: {strategy.power} (polynomial degree: x^power - N)
- Modulus filters: {strategy.modulus_filters} (quick rejection via x mod m)
- Smoothness bound: {strategy.smoothness_bound} (max prime factor to check)
- Min small prime hits: {strategy.min_small_prime_hits} (required small factor count)

**Current Performance**:
- Fitness: {fitness} candidates found in evaluation window
- Recent trend: {fitness_history[-5:] if len(fitness_history) >= 5 else fitness_history}
- Generation: {generation}/{max_generations}

**EvaluationSpecialist's Feedback**:
{feedback_text}

**Domain Knowledge**:
- Higher power → more candidates but slower evaluation
- More filters → fewer candidates but potentially higher quality
- Higher smoothness_bound → more thorough prime checking (slower)
- Higher min_hits → stricter acceptance (fewer candidates)

**Your Task**:
Analyze the feedback and propose a mutation that addresses the issues raised.

**Reasoning Process**:
1. What is the feedback telling you? (performance, quality, efficiency issues?)
2. What is the root cause? (too restrictive? too permissive? inefficient?)
3. What parameter change would address this?
4. What are the tradeoffs of this change?

**Output Format**:
Propose ONE mutation type:
- power: Change polynomial degree
- add_filter: Add modulus filter
- modify_filter: Change existing filter
- remove_filter: Remove filter
- adjust_smoothness: Change smoothness_bound or min_hits

Provide clear reasoning for your choice.
"""

    try:
        # Call Gemini with structured output
        response = self.model.generate_content(
            prompt,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": self.mutation_schema,
                "temperature": temperature,
                "max_output_tokens": self.config.max_tokens,
            }
        )

        # Parse and validate
        result = json.loads(response.text)
        mutation_type = result.get("mutation_type")
        mutation_params = result.get(mutation_type, {})
        reasoning = result.get("reasoning", "No reasoning provided")

        # Track tokens and cost
        input_tokens = getattr(response.usage_metadata, "prompt_token_count", 0)
        output_tokens = getattr(response.usage_metadata, "candidates_token_count", 0)

        return LLMResponse(
            success=True,
            mutation_params={mutation_type: mutation_params},
            reasoning=reasoning,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            provider="gemini",
            model="gemini-2.5-flash-lite"
        )

    except Exception as e:
        logger.error(f"LLM mutation proposal failed: {e}")
        return LLMResponse(
            success=False,
            mutation_params={},
            reasoning=f"Error: {str(e)}",
            input_tokens=0,
            output_tokens=0,
            provider="gemini",
            model="gemini-2.5-flash-lite"
        )
    finally:
        self._call_count += 1
```

**Design Rationale**:
- **Rich context**: Include feedback, fitness history, generation progress
- **Explicit reasoning**: Force LLM to explain its mutation choice
- **Domain knowledge**: Educate LLM about GNFS tradeoffs
- **Structured output**: JSON schema ensures valid mutations
- **Temperature scaling**: Explore early, exploit late

---

**2. SearchSpecialist LLM Integration (prometheus/agents.py) - 4 hours**

**Enhanced SearchSpecialist with LLM Mode**:
```python
class SearchSpecialist(CognitiveCell):
    """Specialized agent for strategy generation and evolution."""

    def __init__(
        self,
        agent_id: str,
        config: Config,
        llm_provider: Optional[LLMProvider] = None
    ):
        super().__init__(agent_id, config)
        self.generator = LLMStrategyGenerator(config, llm_provider) if llm_provider else StrategyGenerator(config)
        self.llm_mode = llm_provider is not None

    def process_request(self, message: Message) -> Message:
        """
        Generate strategy using feedback.

        Two modes:
        - Rule-based: Use feedback heuristics (C1)
        - LLM-guided: Use Gemini reasoning (C2)
        """
        feedback_context = self._extract_feedback_context()

        if message.message_type == "mutation_request":
            # Mutation with parent context
            parent_strategy = Strategy.from_dict(message.payload["parent_strategy"])
            parent_fitness = message.payload["parent_fitness"]
            generation = message.payload["generation"]

            if self.llm_mode and feedback_context:
                # C2: LLM-guided mutation
                strategy = self._generate_llm_guided_strategy(
                    parent=parent_strategy,
                    parent_fitness=parent_fitness,
                    feedback_context=feedback_context,
                    generation=generation
                )
            elif feedback_context:
                # C1: Rule-based mutation with feedback
                strategy = self._generate_feedback_guided_strategy(feedback_context)
            else:
                # Fallback: random
                strategy = self.generator.generate_random_strategy()
        else:
            # Initial strategy request
            strategy = self.generator.generate_random_strategy()

        return Message(
            sender_id=self.agent_id,
            recipient_id=message.sender_id,
            message_type="strategy_response",
            payload={"strategy": strategy.to_dict()},
            timestamp=time.time(),
            conversation_id=message.conversation_id
        )

    def _generate_llm_guided_strategy(
        self,
        parent: Strategy,
        parent_fitness: float,
        feedback_context: List[Dict],
        generation: int
    ) -> Strategy:
        """
        Generate strategy using LLM reasoning about feedback.

        Process:
        1. Extract last feedback from EvaluationSpecialist
        2. Call LLM with parent, fitness, feedback, history
        3. Apply proposed mutation to parent
        4. Validate and normalize result
        """
        # Get last feedback
        last_feedback = feedback_context[-1]
        feedback_text = last_feedback.get("feedback_text", "")

        # Build fitness history
        fitness_history = [f["fitness"] for f in feedback_context[-5:]]

        # Call LLM
        llm_response = self.generator.llm_provider.propose_mutation_with_feedback(
            strategy=parent,
            fitness=parent_fitness,
            feedback_text=feedback_text,
            fitness_history=fitness_history,
            generation=generation,
            max_generations=20  # From config
        )

        if llm_response.success:
            # Apply LLM-proposed mutation
            mutated = self.generator._apply_llm_mutation(
                parent,
                llm_response.mutation_params
            )
            logger.info(f"LLM reasoning: {llm_response.reasoning}")
            return mutated
        else:
            # Fallback to rule-based if LLM fails
            logger.warning(f"LLM failed: {llm_response.reasoning}. Using rule-based fallback.")
            return self._generate_feedback_guided_strategy(feedback_context)
```

**Design Rationale**:
- **Dual mode**: Support both C1 (rule-based) and C2 (LLM) with same interface
- **Graceful fallback**: If LLM fails, use rule-based (robustness)
- **Logging**: Track LLM reasoning for analysis
- **Reuse**: Leverage existing `_apply_llm_mutation` from LLMStrategyGenerator

---

**3. Experiments & Analysis - 8 hours**

**Experiment Protocol**:
```bash
# Treatment: Collaborative with LLM (C2)
for seed in {7000..7009}; do
  python main.py --prometheus --prometheus-mode collaborative --llm \
    --generations 20 --population 20 --duration 1.0 --seed $seed \
    --export-metrics results/c2_validation/collaborative_llm_${seed}.json
done
```

**Cost**: ~10 runs × 20 gen × 20 pop × 0.5 LLM calls/strategy × $0.0004 = ~$1.60

**Statistical Analysis**:
```python
# H1b: LLM collaborative > C1 collaborative
result_h1b = analyzer.compare_fitness_distributions(
    evolved_scores=collaborative_llm_fitness,
    baseline_scores=collaborative_c1_fitness
)

# Also: LLM collaborative vs Rule-based (sanity check)
result_llm_vs_rulebased = analyzer.compare_fitness_distributions(
    evolved_scores=collaborative_llm_fitness,
    baseline_scores=rulebased_fitness
)

# Emergence factors
emergence_c1 = mean(collaborative_c1_fitness) / max(mean(search_only), mean(eval_only), mean(rulebased))
emergence_c2 = mean(collaborative_llm_fitness) / max(mean(search_only), mean(eval_only), mean(rulebased))
```

**Possible Outcomes & Publications**:

**Outcome 1: LLM enhances collaboration** ✅
- p < 0.05, d ≥ 0.5, emergence_c2 > emergence_c1
- **Publication 1**: "Cognitive Specialization Enables Collective Intelligence" (C1 results)
- **Publication 2**: "LLM Reasoning Enhances Multi-Agent Collaboration" (C2 results)

**Outcome 2: LLM doesn't help** ❌ (Still valuable!)
- p ≥ 0.05 or d < 0.3, emergence_c2 ≈ emergence_c1
- **Publication 1**: "Cognitive Specialization in Multi-Agent Systems" (C1 results)
- **Publication 2**: "When Do LLMs Improve Multi-Agent Problem-Solving? A Negative Result" (C2 negative finding)
  - Analysis: Why didn't LLM help? (prompt quality, task nature, feedback structure)
  - Generalizable: Lessons for LLM-agent collaboration

**Outcome 3: LLM makes it worse** ❌❌ (Most interesting!)
- p < 0.05 but d < 0 (negative effect), emergence_c2 < emergence_c1
- **Publication 1**: Same as Outcome 2
- **Publication 2**: "LLM Reasoning Can Harm Multi-Agent Collaboration: An Empirical Study"
  - Deep dive: Why does LLM reasoning degrade performance?
  - Hypotheses: Over-optimization, exploration-exploitation imbalance, feedback misinterpretation
  - Implications: When NOT to use LLMs in agent systems

**All outcomes are publishable research findings.**

---

## Phase 2 (C4): Alternative Collaboration Models - Weeks 4-6

### Strategic Rationale

**Build on validated C3 foundation**:
- C1 validated that collaboration infrastructure works (or identified what doesn't)
- C2 tested whether LLM reasoning helps (positive or negative finding informs C4)

**Explore collaboration protocol design space**:
- H1 (collaboration > independence) tested in C3
- **New questions**:
  - What collaboration structures maximize emergence?
  - Can agents develop division of labor spontaneously?
  - How does communication topology affect collective intelligence?

**Generalize beyond factorization**:
- C4 findings apply to ANY multi-agent problem-solving domain
- Collaboration models transferable to other AI civilization testbeds

---

### C4.1: Negotiation Model

#### Concept
**Agents negotiate strategy parameters through argumentation and counter-proposals.**

**Example dialogue**:
```
SearchSpecialist: "I propose power=4 with filters=[(7, [0,1])]"
EvaluationSpecialist: "Power=4 is too slow (80% time in candidate generation).
                       I counter-propose power=3 with same filters."
SearchSpecialist: "Agreed on power=3, but I suggest adding filter (11, [0,5])
                   to maintain coverage while improving quality."
EvaluationSpecialist: "Acceptable. Testing power=3, filters=[(7,[0,1]), (11,[0,5])]"
```

#### Implementation Design

**New Agent**: `NegotiationMediator`
```python
class NegotiationMediator(CognitiveCell):
    """Mediates negotiation between SearchSpecialist and EvaluationSpecialist."""

    def negotiate_strategy(
        self,
        initial_proposal: Strategy,
        max_rounds: int = 3
    ) -> Strategy:
        """
        Multi-round negotiation protocol.

        Rounds:
        1. SearchSpecialist proposes strategy
        2. EvaluationSpecialist critiques (predict fitness, identify issues)
        3. SearchSpecialist revises or defends
        4. Repeat until agreement or max_rounds
        5. Mediator resolves if no agreement
        """
        proposal = initial_proposal

        for round_num in range(max_rounds):
            # EvaluationSpecialist critique
            critique = self.eval_specialist.critique_strategy(proposal)

            if critique["acceptable"]:
                # Agreement reached
                return proposal

            # SearchSpecialist responds to critique
            counter_proposal = self.search_specialist.revise_strategy(
                proposal, critique
            )

            # Check convergence
            if counter_proposal == proposal:
                # No change: stalemate
                break

            proposal = counter_proposal

        # No agreement: mediator compromises
        return self._mediate_compromise(proposal, critique)
```

**Research Questions**:
- Does negotiation produce better strategies than direct feedback?
- How many negotiation rounds optimal? (cost vs quality)
- Do agents develop persuasive argumentation strategies?

**Metrics**:
- Convergence rate (% reaching agreement)
- Rounds to agreement (communication efficiency)
- Compromise quality (mediator involvement)
- Final strategy fitness vs C2

---

### C4.2: Voting/Ensemble Model

#### Concept
**Multiple SearchSpecialists propose strategies, EvaluationSpecialist ranks them, ensemble combination.**

**Architecture**:
```
SearchSpecialist_1 (conservative) → Strategy A
SearchSpecialist_2 (aggressive)   → Strategy B
SearchSpecialist_3 (balanced)     → Strategy C
                                     ↓
                    EvaluationSpecialist evaluates all
                                     ↓
                    Rank: B (4500) > C (4200) > A (3800)
                                     ↓
                    Ensemble: Crossover best 2 (B+C)
```

#### Implementation Design

**New Component**: `SpecialistEnsemble`
```python
class SpecialistEnsemble:
    """Manages multiple SearchSpecialists with different strategies."""

    def __init__(self, config: Config):
        self.specialists = [
            SearchSpecialist("search_conservative", config, bias="conservative"),
            SearchSpecialist("search_aggressive", config, bias="aggressive"),
            SearchSpecialist("search_balanced", config, bias="balanced"),
        ]
        self.eval_specialist = EvaluationSpecialist("eval", config)

    def generate_ensemble_strategy(self) -> Strategy:
        """
        Ensemble strategy generation.

        Process:
        1. Each specialist proposes strategy
        2. EvaluationSpecialist evaluates all
        3. Select top K (K=2)
        4. Crossover top proposals
        5. Return ensemble strategy
        """
        proposals = []
        for specialist in self.specialists:
            strategy = specialist.generate_strategy()
            fitness = self.eval_specialist.evaluate(strategy)
            proposals.append((strategy, fitness, specialist.agent_id))

        # Rank by predicted fitness
        proposals.sort(key=lambda x: x[1], reverse=True)

        # Ensemble: crossover top 2
        top1, top2 = proposals[0][0], proposals[1][0]
        ensemble = crossover_strategies(top1, top2)

        return ensemble
```

**SearchSpecialist Biases**:
```python
# Conservative: Low power, many filters, high min_hits
{"power": 2-3, "max_filters": 3-4, "min_hits": 3-5}

# Aggressive: High power, few filters, low min_hits
{"power": 4-5, "max_filters": 1-2, "min_hits": 1-2}

# Balanced: Medium everything
{"power": 3, "max_filters": 2-3, "min_hits": 2-3}
```

**Research Questions**:
- Does diversity of specialists improve ensemble quality?
- Optimal ensemble size (2? 3? 5?)
- Voting rule: top-K crossover vs weighted average?
- Specialist specialization over time?

**Metrics**:
- Diversity of proposals (variance in parameters)
- Ensemble benefit (ensemble fitness vs best individual)
- Specialist contribution (which bias wins most often?)

---

### C4.3: Market Model

#### Concept
**SearchSpecialists bid for evaluation resources, EvaluationSpecialist allocates budget, strategies compete.**

**Market Dynamics**:
```
SearchSpecialist_1 confidence=0.9 → bids 100 credits for evaluation
SearchSpecialist_2 confidence=0.6 → bids 60 credits
SearchSpecialist_3 confidence=0.4 → bids 40 credits

EvaluationSpecialist budget=150 credits:
- Allocate 100 to S1 (highest bid)
- Allocate 50 to S2 (partial)
- Reject S3 (insufficient budget)

Post-evaluation:
- S1 strategy fitness=5000 → reward +150 credits (fitness/50)
- S2 strategy fitness=2000 → reward +40 credits
- Update specialist budgets for next generation
```

#### Implementation Design

**New Component**: `MarketOrchestrator`
```python
class MarketOrchestrator:
    """Manages market-based resource allocation."""

    def __init__(self, config: Config, initial_budget: int = 1000):
        self.specialists = [
            SearchSpecialist("search_1", config, budget=initial_budget),
            SearchSpecialist("search_2", config, budget=initial_budget),
            SearchSpecialist("search_3", config, budget=initial_budget),
        ]
        self.eval_specialist = EvaluationSpecialist("eval", config)
        self.eval_budget_per_gen = 200  # Total evaluation credits

    def run_market_generation(self) -> List[Tuple[Strategy, float]]:
        """
        Market-based generation.

        Process:
        1. Specialists propose strategies + bids (confidence)
        2. Rank bids, allocate eval budget
        3. Evaluate allocated strategies
        4. Reward specialists based on fitness
        5. Update budgets for next generation
        """
        # 1. Collect proposals and bids
        proposals = []
        for specialist in self.specialists:
            strategy, bid = specialist.propose_with_bid()
            proposals.append({
                "specialist": specialist,
                "strategy": strategy,
                "bid": bid
            })

        # 2. Allocate evaluation budget
        proposals.sort(key=lambda x: x["bid"], reverse=True)
        allocated = []
        remaining_budget = self.eval_budget_per_gen

        for prop in proposals:
            if prop["bid"] <= remaining_budget:
                allocated.append(prop)
                remaining_budget -= prop["bid"]

        # 3. Evaluate allocated strategies
        results = []
        for prop in allocated:
            fitness = self.eval_specialist.evaluate(prop["strategy"])
            results.append({
                "specialist": prop["specialist"],
                "strategy": prop["strategy"],
                "fitness": fitness,
                "cost": prop["bid"]
            })

        # 4. Reward specialists
        for result in results:
            reward = result["fitness"] / 50  # Fitness-based reward
            result["specialist"].budget += reward - result["cost"]

        return [(r["strategy"], r["fitness"]) for r in results]
```

**SearchSpecialist Bidding Strategy**:
```python
def propose_with_bid(self) -> Tuple[Strategy, int]:
    """
    Propose strategy and bid for evaluation.

    Bid = f(confidence, budget, desperation)
    - Confidence: How good I think this strategy is
    - Budget: How much I can afford
    - Desperation: How long since my last success
    """
    strategy = self.generate_strategy()

    # Estimate confidence (simple heuristic)
    confidence = self._estimate_quality(strategy)  # 0.0-1.0

    # Budget constraint
    affordable = self.budget * 0.3  # Max 30% of budget per bid

    # Desperation factor
    generations_since_success = ...
    desperation = min(1.5, 1.0 + 0.1 * generations_since_success)

    # Final bid
    bid = int(affordable * confidence * desperation)

    return strategy, bid
```

**Research Questions**:
- Do specialists learn to bid strategically?
- Does market allocation improve resource efficiency?
- Emergent specialization (some specialists always bid high/low)?
- Wealth inequality effects (rich get richer)?

**Metrics**:
- Budget distribution over time (inequality)
- Bid accuracy (correlation between bid and actual fitness)
- Resource efficiency (total fitness / total evaluation budget)
- Market stability (budget variance)

---

### C4.4: Cognitive Division of Labor

#### Concept
**Specialists spontaneously develop roles: exploration, exploitation, theory, meta-learning.**

**Agent Types**:
```
ExplorationSpecialist: Generates novel, risky strategies
ExploitationSpecialist: Refines known-good strategies
TheorySpecialist: Analyzes why strategies work, builds domain models
MetaSpecialist: Orchestrates others, allocates attention
```

**Emergence Protocol**:
```
Generation 0-5: All agents generalists (same behavior)
Generation 6+: Agents differentiate based on success patterns
  - Agent with high variance → ExplorationSpecialist
  - Agent with incremental improvements → ExploitationSpecialist
  - Agent with good explanations → TheorySpecialist
  - Agent with good coordination → MetaSpecialist
```

#### Implementation Design

**Adaptive Role Selection**:
```python
class AdaptiveSpecialist(CognitiveCell):
    """Specialist that adapts role based on performance history."""

    def __init__(self, agent_id: str, config: Config):
        super().__init__(agent_id, config)
        self.role = "generalist"  # Initial role
        self.performance_history = []

    def update_role(self):
        """
        Adapt role based on performance patterns.

        Role selection criteria:
        - Exploration: High variance in fitness, some very high peaks
        - Exploitation: Low variance, consistent improvement
        - Theory: Good at predicting fitness, explaining failures
        - Meta: Good at orchestrating, allocating resources
        """
        if len(self.performance_history) < 5:
            return  # Need more data

        recent = self.performance_history[-10:]
        mean_fitness = np.mean([p["fitness"] for p in recent])
        std_fitness = np.std([p["fitness"] for p in recent])

        # Calculate role scores
        exploration_score = std_fitness / (mean_fitness + 1)  # High variance
        exploitation_score = 1 / (std_fitness + 1)  # Low variance

        # Assign role
        if exploration_score > 0.5:
            self.role = "exploration"
        elif exploitation_score > 0.8:
            self.role = "exploitation"
        else:
            self.role = "generalist"

        logger.info(f"{self.agent_id} adopted role: {self.role}")

    def generate_strategy(self, **kwargs) -> Strategy:
        """Generate strategy according to current role."""
        if self.role == "exploration":
            return self._generate_exploratory_strategy()
        elif self.role == "exploitation":
            return self._generate_exploitative_strategy(kwargs.get("best_known"))
        else:
            return self._generate_balanced_strategy()
```

**Orchestration with MetaSpecialist**:
```python
class MetaSpecialist(CognitiveCell):
    """Orchestrates division of labor among specialists."""

    def allocate_generation_budget(
        self,
        specialists: List[AdaptiveSpecialist],
        population_size: int
    ) -> Dict[str, int]:
        """
        Allocate generation budget based on current needs.

        Early generations: More exploration
        Late generations: More exploitation
        Stagnation detected: Increase exploration
        """
        # Count roles
        role_counts = {"exploration": 0, "exploitation": 0, "generalist": 0}
        for s in specialists:
            role_counts[s.role] += 1

        # Determine allocation
        if self._detect_stagnation():
            # Stagnation: need more exploration
            allocation = {
                "exploration": int(population_size * 0.5),
                "exploitation": int(population_size * 0.2),
                "generalist": int(population_size * 0.3)
            }
        elif self.generation < self.max_generations * 0.3:
            # Early: exploration-heavy
            allocation = {
                "exploration": int(population_size * 0.6),
                "exploitation": int(population_size * 0.1),
                "generalist": int(population_size * 0.3)
            }
        else:
            # Late: exploitation-heavy
            allocation = {
                "exploration": int(population_size * 0.2),
                "exploitation": int(population_size * 0.5),
                "generalist": int(population_size * 0.3)
            }

        return allocation
```

**Research Questions**:
- Do agents spontaneously develop stable roles?
- Optimal role distribution (exploration/exploitation balance)?
- Does division of labor improve collective performance?
- Can agents switch roles adaptively?

**Metrics**:
- Role stability (how often agents switch roles)
- Role distribution over time (convergence to equilibrium?)
- Fitness contribution by role (which role contributes most?)
- Diversity maintenance (does exploration prevent premature convergence?)

---

### C4 Experimental Protocol

**Incremental Implementation**:
```
Week 4: Implement C4.1 (Negotiation) + C4.2 (Ensemble)
Week 5: Implement C4.3 (Market) + C4.4 (Division of Labor)
Week 6: Run experiments, analyze, document
```

**Experiments per Model**:
- 10 runs per model × 4 models = 40 runs
- Compare each C4 model against C2 (LLM collaborative)
- Cost: Depends on C4 design (LLM vs rule-based)

**Statistical Analysis**:
```python
# Compare each C4 model against C2 baseline
models = ["negotiation", "ensemble", "market", "division_of_labor"]
for model in models:
    result = analyzer.compare_fitness_distributions(
        evolved_scores=c4_model_fitness[model],
        baseline_scores=c2_llm_fitness
    )
    print(f"{model}: p={result.p_value:.4f}, d={result.effect_size:.2f}")
```

**Publications from C4**:
- **Publication 3**: "Collaboration Protocol Design in Multi-Agent Systems"
  - Systematic comparison of 4 collaboration models
  - Design principles for agent communication
  - Generalize findings beyond factorization
- **Publication 4**: "Emergent Division of Labor in AI Civilizations" (if C4.4 shows emergence)
  - Spontaneous role differentiation
  - Conditions for stable specialization
  - Implications for AI safety (unintended role emergence)

---

## Decision Points & Risk Management

### Phase 1 (C3) Decision Points

**Decision 1: End of Week 1 (C1 Results)**
```
IF H1a SUCCESS (p<0.05, d≥0.5, EF>1.1):
  → Proceed to C2 (LLM enhancement)
  → Publication 1 confirmed (collaboration works)

ELIF H1a MARGINAL (p<0.1, d≥0.3, EF>1.05):
  → Improve C1 (better feedback heuristics)
  → Re-run experiments
  → Delay C2 by 3-5 days

ELSE H1a FAILURE (p≥0.1, d<0.3, EF<1.05):
  → Debug collaboration mechanism
  → Check: Is feedback used? Are mutations guided?
  → Fix and retry C1
  → If still fails: Rethink architecture
```

**Decision 2: End of Week 3 (C2 Results)**
```
IF H1b SUCCESS (LLM > C1):
  → Proceed to C4 with LLM-enhanced C2 as baseline
  → Publication 2 confirmed (LLM helps)

ELIF H1b NEUTRAL (LLM ≈ C1):
  → Proceed to C4 with C1 (rule-based) as baseline
  → Publication 2: Negative finding (LLM doesn't help)
  → C4 uses rule-based (cheaper)

ELSE H1b NEGATIVE (LLM < C1):
  → Proceed to C4 with C1 as baseline
  → Publication 2: LLM harms collaboration (most interesting!)
  → Deep dive: Why? Over-optimization? Bad prompts?
```

**Key Insight**: ALL outcomes enable proceeding to C4. We learn regardless.

---

### Phase 2 (C4) Decision Points

**Decision 3: After C4 Experiments**
```
IF ANY C4 model > C2:
  → Publication 3: "Superior Collaboration Protocol Identified"
  → Deep dive: Why does this protocol work better?

ELIF ALL C4 ≈ C2:
  → Publication 3: "Collaboration Protocol Comparison Study"
  → Finding: Simple feedback sufficient, complex protocols not needed

ELSE ALL C4 < C2:
  → Publication 3: "When Complex Protocols Fail"
  → Analysis: Overhead vs benefit, sweet spot identification
```

**Decision 4: Future Research Direction**
```
After C4 complete, choose next research direction:

Option 1: Apply validated collaboration model to NEW domain
  - Different testbed (e.g., TSP, knapsack, game playing)
  - Test generalizability of collaboration findings

Option 2: Scale to MANY agents (>10)
  - Test emergent organization in large populations
  - Communication topology effects

Option 3: Human-AI collaboration
  - Add human expert as specialist
  - Test AI-human collective intelligence

Option 4: Adversarial multi-agent (competition + collaboration)
  - Some agents compete, some cooperate
  - Game theory of AI civilizations
```

---

## Budget & Timeline Summary

### Phase 1 (C3): Weeks 1-3

| Week | Tasks | API Cost | Deliverables |
|------|-------|----------|--------------|
| 1 (C1) | Rule-based collaboration | $0 | Feedback integration, selection pressure, 10 experiments |
| 2-3 (C2) | LLM enhancement | ~$5 | LLM prompts, 10 experiments, statistical analysis |
| **Total** | **C3 complete** | **~$5** | **Publications 1-2, validated foundation for C4** |

### Phase 2 (C4): Weeks 4-6

| Week | Tasks | API Cost | Deliverables |
|------|-------|----------|--------------|
| 4 | Negotiation + Ensemble | $0-2 | 2 models implemented, 20 experiments |
| 5 | Market + Division of Labor | $0-2 | 2 models implemented, 20 experiments |
| 6 | Analysis & writing | $0 | Statistical comparison, visualization, Publications 3-4 |
| **Total** | **C4 complete** | **$0-4** | **Publications 3-4, novel collaboration models** |

### Grand Total

- **Timeline**: 6 weeks (4-6 weeks accounting for delays)
- **Budget**: $5-9 (well under any reasonable limit)
- **Publications**: 2-4 papers on multi-agent cognitive systems
- **Research Value**: Core AI civilization questions answered

---

## Success Metrics

### Scientific Success
- ✅ H1a tested (collaboration vs independence)
- ✅ H1b tested (LLM enhancement)
- ✅ 2-4 publishable papers (positive OR negative findings)
- ✅ Generalizable collaboration principles identified

### Technical Success
- ✅ Validated multi-agent framework (458+ tests)
- ✅ Reusable collaboration protocols (4 models implemented)
- ✅ Statistical analysis pipeline (reproducible experiments)
- ✅ Visualization infrastructure (publication-quality figures)

### AI Civilization Success
- ✅ Proof of concept: Agents CAN collaborate effectively (if H1a succeeds)
- ✅ Understanding of: When does collaboration help? What protocols work?
- ✅ Foundation for: Scaling to many agents, new domains, human-AI teams
- ✅ Research integrity: Negative results published, honest reporting

---

## Key Principles for Execution

### 1. Research Over Optimization
**Remember**: We're testing AI civilization hypotheses, not optimizing factorization.
- Negative results are findings (publish them!)
- Exploration > Efficiency (try risky ideas)
- Generalizability > Performance (insights that transfer)

### 2. Progressive Validation
**Build confidence incrementally**:
- C1 (rule-based) before C2 (LLM) - validate architecture first
- C3 (foundation) before C4 (exploration) - proven baseline enables creativity
- Publish early, publish often - don't wait for "perfect" results

### 3. Honest Reporting
**Scientific integrity**:
- Pre-register hypotheses (document in plan before experiments)
- Report all outcomes (don't cherry-pick)
- Document failures (why didn't it work?)
- Publish negative results (LLM doesn't help = important finding)

### 4. Systematic Experimentation
**Rigor**:
- Statistical tests (Welch's t-test, Cohen's d, CI)
- Multiple runs (n≥10 for power)
- Control comparisons (always compare to baseline)
- Reproducibility (seeds, configs exported)

### 5. Adaptive Planning
**Flexibility**:
- Decision points built into plan (Week 1 end, Week 3 end)
- Multiple paths forward (success, marginal, failure)
- Budget for exploration (C4 models can be added/removed)
- Learn from failures (iterate on feedback heuristics)

---

## Conclusion

This plan advances AI civilization research through **systematic exploration of multi-agent collaboration**:

**Phase 1 (C3)** tests the core hypothesis:
- H1a: Does cognitive specialization produce emergence?
- H1b: Does LLM reasoning enhance collaboration?

**Phase 2 (C4)** explores the design space:
- What collaboration protocols maximize collective intelligence?
- Can agents develop division of labor spontaneously?
- How does communication structure affect emergence?

**All paths lead to publishable research**:
- Positive results: Proof of concept for AI civilizations
- Negative results: Important findings about collaboration limits
- Null results: Baseline validation for future work

**Timeline**: 6 weeks
**Budget**: $5-9
**Publications**: 2-4 papers
**Foundation**: Validated multi-agent framework for future AI civilization experiments

**Next session**: Begin Week 1 (C1 implementation) - feedback integration + selection pressure.

---

**Document Created**: November 2, 2025
**Purpose**: Strategic plan for AI civilization research via Prometheus Phases 1-2
**Status**: Ready for execution
**First Task**: Implement feedback integration in SearchSpecialist (agents.py:157)
