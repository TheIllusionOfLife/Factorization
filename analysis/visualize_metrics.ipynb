{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution Metrics Visualization\n",
    "\n",
    "This notebook visualizes the detailed metrics collected during evolutionary runs.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, run an evolution with metrics export:\n",
    "```bash\n",
    "python prototype.py --generations 5 --population 10 --duration 0.1 --export-metrics metrics/run.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Metrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metrics file\n",
    "metrics_file = '../metrics/test_run.json'  # Change this to your metrics file\n",
    "\n",
    "with open(metrics_file) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Target Number: {data['target_number']}\")\n",
    "print(f\"Generations: {data['generation_count']}\")\n",
    "print(f\"Population Size: {data['population_size']}\")\n",
    "print(f\"Evaluation Duration: {data['evaluation_duration']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitness Over Generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fitness data\n",
    "generations = range(data['generation_count'])\n",
    "fitness_by_gen = [\n",
    "    [metrics['candidate_count'] for metrics in generation]\n",
    "    for generation in data['metrics_history']\n",
    "]\n",
    "\n",
    "max_fitness = [max(gen) for gen in fitness_by_gen]\n",
    "avg_fitness = [np.mean(gen) for gen in fitness_by_gen]\n",
    "min_fitness = [min(gen) for gen in fitness_by_gen]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(generations, max_fitness, 'b-', label='Best Fitness', linewidth=2.5, marker='o')\n",
    "plt.plot(generations, avg_fitness, 'g--', label='Average Fitness', linewidth=2, marker='s')\n",
    "plt.fill_between(generations, min_fitness, max_fitness, alpha=0.2)\n",
    "\n",
    "plt.xlabel('Generation', fontsize=12)\n",
    "plt.ylabel('Fitness (candidates found)', fontsize=12)\n",
    "plt.title('Evolutionary Progress: Fitness Over Generations', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFitness improvement: {max_fitness[0]} → {max_fitness[-1]} \"+\n",
    "      f\"({((max_fitness[-1]/max(max_fitness[0], 1) - 1) * 100):.1f}% change)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Breakdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average timing percentages across all generations\n",
    "timing_categories = ['candidate_generation', 'modulus_filtering', 'smoothness_check']\n",
    "\n",
    "avg_timings = {cat: [] for cat in timing_categories}\n",
    "\n",
    "for generation in data['metrics_history']:\n",
    "    gen_timings = {cat: [] for cat in timing_categories}\n",
    "    \n",
    "    for metrics in generation:\n",
    "        total_time = sum(metrics['timing_breakdown'].values())\n",
    "        if total_time > 0:\n",
    "            for cat in timing_categories:\n",
    "                pct = (metrics['timing_breakdown'][cat] / total_time) * 100\n",
    "                gen_timings[cat].append(pct)\n",
    "    \n",
    "    for cat in timing_categories:\n",
    "        avg_timings[cat].append(np.mean(gen_timings[cat]) if gen_timings[cat] else 0)\n",
    "\n",
    "# Plot stacked area chart\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.stackplot(generations, \n",
    "              avg_timings['candidate_generation'],\n",
    "              avg_timings['modulus_filtering'],\n",
    "              avg_timings['smoothness_check'],\n",
    "              labels=['Candidate Generation', 'Modulus Filtering', 'Smoothness Check'],\n",
    "              alpha=0.8)\n",
    "\n",
    "plt.xlabel('Generation', fontsize=12)\n",
    "plt.ylabel('Time Allocation (%)', fontsize=12)\n",
    "plt.title('Time Allocation Across Evaluation Phases', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper left', fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 100)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rejection Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze rejection patterns\n",
    "rejection_data = {\n",
    "    'modulus_filter': [],\n",
    "    'min_hits': [],\n",
    "    'passed': []\n",
    "}\n",
    "\n",
    "for generation in data['metrics_history']:\n",
    "    gen_rejections = {'modulus_filter': 0, 'min_hits': 0, 'passed': 0}\n",
    "    \n",
    "    for metrics in generation:\n",
    "        for key in rejection_data.keys():\n",
    "            gen_rejections[key] += metrics['rejection_stats'][key]\n",
    "    \n",
    "    total = sum(gen_rejections.values())\n",
    "    if total > 0:\n",
    "        for key in rejection_data.keys():\n",
    "            rejection_data[key].append((gen_rejections[key] / total) * 100)\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Stacked bar chart\n",
    "x = np.arange(len(generations))\n",
    "width = 0.6\n",
    "\n",
    "ax1.bar(x, rejection_data['modulus_filter'], width, label='Modulus Filter Rejection', alpha=0.8)\n",
    "ax1.bar(x, rejection_data['min_hits'], width, bottom=rejection_data['modulus_filter'],\n",
    "        label='Min Hits Rejection', alpha=0.8)\n",
    "ax1.bar(x, rejection_data['passed'], width, \n",
    "        bottom=np.array(rejection_data['modulus_filter']) + np.array(rejection_data['min_hits']),\n",
    "        label='Passed', alpha=0.8, color='green')\n",
    "\n",
    "ax1.set_xlabel('Generation', fontsize=11)\n",
    "ax1.set_ylabel('Percentage (%)', fontsize=11)\n",
    "ax1.set_title('Rejection vs Acceptance Rates', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(generations)\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Line plot for passed percentage\n",
    "ax2.plot(generations, rejection_data['passed'], 'g-', marker='o', linewidth=2.5, markersize=8)\n",
    "ax2.fill_between(generations, rejection_data['passed'], alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Generation', fontsize=11)\n",
    "ax2.set_ylabel('Pass Rate (%)', fontsize=11)\n",
    "ax2.set_title('Evolution of Pass Rate', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPass rate improvement: {rejection_data['passed'][0]:.2f}% → {rejection_data['passed'][-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothness Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze smoothness scores (lower = smoother)\n",
    "avg_smoothness_by_gen = []\n",
    "\n",
    "for generation in data['metrics_history']:\n",
    "    all_scores = []\n",
    "    for metrics in generation:\n",
    "        if metrics['smoothness_scores']:\n",
    "            # Filter out inf values\n",
    "            valid_scores = [s for s in metrics['smoothness_scores'] if np.isfinite(s)]\n",
    "            all_scores.extend(valid_scores)\n",
    "    \n",
    "    if all_scores:\n",
    "        # Use log scale for better visualization\n",
    "        avg_smoothness_by_gen.append(np.log10(np.mean(all_scores)))\n",
    "    else:\n",
    "        avg_smoothness_by_gen.append(None)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "valid_gens = [g for g, s in zip(generations, avg_smoothness_by_gen) if s is not None]\n",
    "valid_scores = [s for s in avg_smoothness_by_gen if s is not None]\n",
    "\n",
    "plt.plot(valid_gens, valid_scores, 'r-', marker='o', linewidth=2.5, markersize=8)\n",
    "plt.xlabel('Generation', fontsize=12)\n",
    "plt.ylabel('Log10(Average Smoothness Ratio)', fontsize=12)\n",
    "plt.title('Smoothness Quality Evolution (Lower = Better)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "if len(valid_scores) >= 2:\n",
    "    print(f\"\\nSmoothness trend: {'Improving (↓)' if valid_scores[-1] < valid_scores[0] else 'Degrading (↑)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Strategy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best strategy from last generation\n",
    "last_gen = data['metrics_history'][-1]\n",
    "best_idx = max(range(len(last_gen)), key=lambda i: last_gen[i]['candidate_count'])\n",
    "best_metrics = last_gen[best_idx]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST STRATEGY FROM FINAL GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFitness: {best_metrics['candidate_count']} candidates found\")\n",
    "print(f\"\\nTiming Breakdown:\")\n",
    "total_time = sum(best_metrics['timing_breakdown'].values())\n",
    "for phase, time_val in best_metrics['timing_breakdown'].items():\n",
    "    pct = (time_val / total_time * 100) if total_time > 0 else 0\n",
    "    print(f\"  {phase.replace('_', ' ').title()}: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nRejection Stats:\")\n",
    "total_attempts = sum(best_metrics['rejection_stats'].values())\n",
    "for stat, count in best_metrics['rejection_stats'].items():\n",
    "    pct = (count / total_attempts * 100) if total_attempts > 0 else 0\n",
    "    print(f\"  {stat.replace('_', ' ').title()}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "if best_metrics['smoothness_scores']:\n",
    "    valid_scores = [s for s in best_metrics['smoothness_scores'] if np.isfinite(s)]\n",
    "    if valid_scores:\n",
    "        print(f\"\\nSmoothness Quality:\")\n",
    "        print(f\"  Average: {np.mean(valid_scores):.2e}\")\n",
    "        print(f\"  Min: {np.min(valid_scores):.2e}\")\n",
    "        print(f\"  Max: {np.max(valid_scores):.2e}\")\n",
    "\n",
    "if best_metrics['example_candidates']:\n",
    "    print(f\"\\nExample Smooth Candidates:\")\n",
    "    for i, candidate in enumerate(best_metrics['example_candidates'][:3], 1):\n",
    "        print(f\"  {i}. {candidate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVOLUTION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_candidates_found = sum(\n",
    "    metrics['candidate_count']\n",
    "    for generation in data['metrics_history']\n",
    "    for metrics in generation\n",
    ")\n",
    "\n",
    "total_evaluations = data['generation_count'] * data['population_size']\n",
    "total_time = total_evaluations * data['evaluation_duration']\n",
    "\n",
    "print(f\"\\nTotal Evaluations: {total_evaluations}\")\n",
    "print(f\"Total Candidates Found: {total_candidates_found:,}\")\n",
    "print(f\"Average per Evaluation: {total_candidates_found / total_evaluations:.1f}\")\n",
    "print(f\"Total Evaluation Time: {total_time:.1f}s\")\n",
    "print(f\"Candidates per Second: {total_candidates_found / total_time:.1f}\")\n",
    "\n",
    "improvement = ((max_fitness[-1] / max(max_fitness[0], 1)) - 1) * 100\n",
    "print(f\"\\nBest Fitness Improvement: {improvement:+.1f}%\")\n",
    "print(f\"Final Best Strategy: {max_fitness[-1]:,} candidates\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
