{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C1 Validation Exploration Notebook\n",
    "\n",
    "Interactive analysis of C1 validation results for Hypothesis H1a.\n",
    "\n",
    "**Hypothesis**: Collaborative mode with feedback-guided mutations outperforms baselines.\n",
    "\n",
    "**Status**: ❌ NOT SUPPORTED (emergence=0.954, p=0.579, d=-0.58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style for publication-quality plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load statistical analysis results\n",
    "data_path = Path(\"../results/c1_validation/h1a_analysis.json\")\n",
    "with open(data_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract fitness data\n",
    "collaborative = np.array(data[\"collaborative_fitness\"])\n",
    "search_only = np.array(data[\"search_only_fitness\"])\n",
    "rulebased = np.array(data[\"rulebased_fitness\"])\n",
    "\n",
    "# Extract metrics\n",
    "metrics = data[\"metrics\"]\n",
    "stats_tests = data[\"statistical_tests\"]\n",
    "ci = data[\"confidence_intervals\"]\n",
    "h1a_criteria = data[\"h1a_criteria\"]\n",
    "\n",
    "print(\"✓ Data loaded successfully\")\n",
    "print(f\"  Collaborative runs: {len(collaborative)}\")\n",
    "print(f\"  Search-only runs: {len(search_only)}\")\n",
    "print(f\"  Rule-based runs: {len(rulebased)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_mode(data, name):\n",
    "    \"\"\"Print descriptive statistics for a mode.\"\"\"\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean: {data.mean():.1f}\")\n",
    "    print(f\"  Median: {np.median(data):.1f}\")\n",
    "    print(f\"  Std Dev: {data.std(ddof=1):.1f}\")\n",
    "    print(f\"  Min: {data.min():.1f}\")\n",
    "    print(f\"  Max: {data.max():.1f}\")\n",
    "    print(f\"  Range: {data.max() - data.min():.1f}\")\n",
    "    print(f\"  CV: {(data.std(ddof=1) / data.mean()) * 100:.1f}%\")\n",
    "\n",
    "\n",
    "describe_mode(collaborative, \"Collaborative\")\n",
    "describe_mode(search_only, \"Search-only\")\n",
    "describe_mode(rulebased, \"Rule-based\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Histogram comparison\n",
    "axes[0].hist(\n",
    "    [collaborative, search_only, rulebased],\n",
    "    label=[\"Collaborative\", \"Search-only\", \"Rule-based\"],\n",
    "    bins=8,\n",
    "    alpha=0.6,\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[0].set_xlabel(\"Final Fitness\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "axes[0].set_title(\"Distribution Comparison\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(\n",
    "    [collaborative, search_only, rulebased],\n",
    "    labels=[\"Collaborative\", \"Search-only\", \"Rule-based\"],\n",
    ")\n",
    "axes[1].set_ylabel(\"Final Fitness\")\n",
    "axes[1].set_title(\"Box Plot Comparison\")\n",
    "axes[1].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Violin plot\n",
    "positions = [1, 2, 3]\n",
    "parts = axes[2].violinplot(\n",
    "    [collaborative, search_only, rulebased],\n",
    "    positions=positions,\n",
    "    showmeans=True,\n",
    "    showmedians=True,\n",
    ")\n",
    "axes[2].set_xticks(positions)\n",
    "axes[2].set_xticklabels([\"Collaborative\", \"Search-only\", \"Rule-based\"])\n",
    "axes[2].set_ylabel(\"Final Fitness\")\n",
    "axes[2].set_title(\"Violin Plot Comparison\")\n",
    "axes[2].grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Normality Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shapiro-Wilk test for normality\n",
    "def test_normality(data, name):\n",
    "    \"\"\"Test normality using Shapiro-Wilk test.\"\"\"\n",
    "    stat, p = stats.shapiro(data)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  W-statistic: {stat:.4f}\")\n",
    "    print(f\"  p-value: {p:.4f}\")\n",
    "    if p > 0.05:\n",
    "        print(f\"  ✓ Normality not rejected (p={p:.4f} > 0.05)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Normality rejected (p={p:.4f} < 0.05)\")\n",
    "\n",
    "\n",
    "print(\"Shapiro-Wilk Normality Tests:\")\n",
    "test_normality(collaborative, \"Collaborative\")\n",
    "test_normality(search_only, \"Search-only\")\n",
    "test_normality(rulebased, \"Rule-based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plots to visually assess normality\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for ax, data_arr, name in zip(\n",
    "    axes,\n",
    "    [collaborative, search_only, rulebased],\n",
    "    [\"Collaborative\", \"Search-only\", \"Rule-based\"],\n",
    "):\n",
    "    stats.probplot(data_arr, dist=\"norm\", plot=ax)\n",
    "    ax.set_title(f\"Q-Q Plot: {name}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welch's t-test (collaborative vs. rule-based)\n",
    "t_stat, p_value = stats.ttest_ind(collaborative, rulebased, equal_var=False)\n",
    "\n",
    "print(\"Welch's t-test (Collaborative vs. Rule-based):\")\n",
    "print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4f}\")\n",
    "print(f\"  Significant at α=0.05: {p_value < 0.05}\")\n",
    "\n",
    "# Cohen's d effect size\n",
    "pooled_std = np.sqrt((collaborative.std(ddof=1) ** 2 + rulebased.std(ddof=1) ** 2) / 2)\n",
    "cohens_d = (collaborative.mean() - rulebased.mean()) / pooled_std\n",
    "\n",
    "print(f\"\\nCohen's d effect size: {cohens_d:.4f}\")\n",
    "if abs(cohens_d) < 0.2:\n",
    "    print(\"  Interpretation: Small effect\")\n",
    "elif abs(cohens_d) < 0.5:\n",
    "    print(\"  Interpretation: Small-to-medium effect\")\n",
    "elif abs(cohens_d) < 0.8:\n",
    "    print(\"  Interpretation: Medium effect\")\n",
    "else:\n",
    "    print(\"  Interpretation: Large effect\")\n",
    "\n",
    "if cohens_d < 0:\n",
    "    print(\"  Direction: Collaborative UNDERPERFORMED rule-based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three modes with ANOVA\n",
    "f_stat, p_anova = stats.f_oneway(collaborative, search_only, rulebased)\n",
    "\n",
    "print(\"\\nOne-way ANOVA (all three modes):\")\n",
    "print(f\"  F-statistic: {f_stat:.4f}\")\n",
    "print(f\"  p-value: {p_anova:.4f}\")\n",
    "print(f\"  Significant at α=0.05: {p_anova < 0.05}\")\n",
    "\n",
    "if p_anova < 0.05:\n",
    "    print(\"  ✓ At least one mode differs significantly from others\")\n",
    "else:\n",
    "    print(\"  ⚠ No significant difference between modes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 95% confidence intervals\n",
    "def calculate_ci(data, confidence=0.95):\n",
    "    \"\"\"Calculate confidence interval using t-distribution.\"\"\"\n",
    "    n = len(data)\n",
    "    mean = data.mean()\n",
    "    se = stats.sem(data)\n",
    "    margin = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean - margin, mean + margin\n",
    "\n",
    "\n",
    "ci_collaborative = calculate_ci(collaborative)\n",
    "ci_search_only = calculate_ci(search_only)\n",
    "ci_rulebased = calculate_ci(rulebased)\n",
    "\n",
    "print(\"95% Confidence Intervals:\")\n",
    "print(f\"  Collaborative: [{ci_collaborative[0]:.1f}, {ci_collaborative[1]:.1f}]\")\n",
    "print(f\"  Search-only:   [{ci_search_only[0]:.1f}, {ci_search_only[1]:.1f}]\")\n",
    "print(f\"  Rule-based:    [{ci_rulebased[0]:.1f}, {ci_rulebased[1]:.1f}]\")\n",
    "\n",
    "# Check for overlap\n",
    "overlap_start = max(ci_collaborative[0], ci_rulebased[0])\n",
    "overlap_end = min(ci_collaborative[1], ci_rulebased[1])\n",
    "overlap = max(0, overlap_end - overlap_start)\n",
    "total_range = max(ci_collaborative[1], ci_rulebased[1]) - min(\n",
    "    ci_collaborative[0], ci_rulebased[0]\n",
    ")\n",
    "overlap_pct = (overlap / total_range) * 100\n",
    "\n",
    "print(f\"\\nCI overlap (Collaborative vs. Rule-based): {overlap_pct:.1f}%\")\n",
    "if overlap_pct < 5:\n",
    "    print(\"  Strong evidence of difference\")\n",
    "elif overlap_pct < 25:\n",
    "    print(\"  Moderate evidence of difference\")\n",
    "else:\n",
    "    print(\"  Weak evidence of difference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Emergence Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate emergence factor\n",
    "emergence = metrics[\"emergence_factor\"]\n",
    "synergy = metrics[\"synergy_score\"]\n",
    "improvement_pct = metrics[\"improvement_pct\"]\n",
    "\n",
    "print(\"Emergence Analysis:\")\n",
    "print(f\"  Emergence factor: {emergence:.4f}\")\n",
    "print(\"  Target: >1.1 (10% improvement)\")\n",
    "print(f\"  Status: {'✓ PASSED' if emergence > 1.1 else '❌ FAILED'}\")\n",
    "print(f\"\\n  Synergy score: {synergy:.1f}\")\n",
    "print(f\"  Improvement: {improvement_pct:.2f}%\")\n",
    "\n",
    "if emergence < 1.0:\n",
    "    print(f\"\\n  ⚠ Collaborative UNDERPERFORMED by {(1 - emergence) * 100:.1f}%\")\n",
    "elif emergence < 1.1:\n",
    "    print(\n",
    "        f\"\\n  ⚠ Collaborative improved by {(emergence - 1) * 100:.1f}%, but below 10% target\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\\n  ✓ Collaborative exceeded target by {(emergence - 1.1) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize emergence factor\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "modes = [\"Collaborative\", \"Rule-based\\n(baseline)\", \"Emergence\\nTarget\"]\n",
    "values = [\n",
    "    metrics[\"collaborative_mean\"],\n",
    "    metrics[\"rulebased_mean\"],\n",
    "    metrics[\"rulebased_mean\"] * 1.1,\n",
    "]\n",
    "colors = [\"#e74c3c\" if emergence < 1.1 else \"#2ecc71\", \"#95a5a6\", \"#3498db\"]\n",
    "\n",
    "bars = ax.bar(modes, values, color=colors, alpha=0.7, edgecolor=\"black\", linewidth=1.5)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        height,\n",
    "        f\"{val:.0f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_ylabel(\"Mean Final Fitness\", fontweight=\"bold\")\n",
    "ax.set_title(\n",
    "    f\"Emergence Factor: {emergence:.3f} (Target: >1.1)\", fontweight=\"bold\", fontsize=14\n",
    ")\n",
    "ax.axhline(\n",
    "    y=metrics[\"rulebased_mean\"],\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.5,\n",
    "    label=\"Baseline\",\n",
    ")\n",
    "ax.axhline(\n",
    "    y=metrics[\"rulebased_mean\"] * 1.1,\n",
    "    color=\"blue\",\n",
    "    linestyle=\"--\",\n",
    "    alpha=0.5,\n",
    "    label=\"Target (+10%)\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrospective power analysis\n",
    "from statsmodels.stats.power import ttest_power\n",
    "\n",
    "# Observed effect size\n",
    "observed_d = abs(cohens_d)\n",
    "n_per_group = len(collaborative)\n",
    "\n",
    "# Calculate achieved power\n",
    "achieved_power = ttest_power(\n",
    "    observed_d, n_per_group, alpha=0.05, alternative=\"two-sided\"\n",
    ")\n",
    "\n",
    "print(\"Retrospective Power Analysis:\")\n",
    "print(f\"  Observed effect size (d): {observed_d:.4f}\")\n",
    "print(f\"  Sample size per group: {n_per_group}\")\n",
    "print(f\"  Achieved power: {achieved_power:.2f}\")\n",
    "print(\"  Target power: 0.80\")\n",
    "\n",
    "if achieved_power < 0.80:\n",
    "    print(f\"\\n  ⚠ Study was UNDERPOWERED (power={achieved_power:.2f} < 0.80)\")\n",
    "\n",
    "    # Calculate required sample size for 80% power\n",
    "    from statsmodels.stats.power import tt_ind_solve_power\n",
    "\n",
    "    required_n = tt_ind_solve_power(\n",
    "        effect_size=observed_d, alpha=0.05, power=0.80, alternative=\"two-sided\"\n",
    "    )\n",
    "    print(f\"  Required N per group for 80% power: {int(np.ceil(required_n))}\")\n",
    "else:\n",
    "    print(\"\\n  ✓ Study was adequately powered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hypothesis Testing Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display H1a criteria results\n",
    "print(\"H1a Hypothesis Testing Summary:\")\n",
    "print(\"\\nSuccess Criteria:\")\n",
    "print(\n",
    "    f\"  1. Emergence factor >1.1:  {h1a_criteria['emergence_factor']} (observed: {emergence:.3f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  2. Significance p<0.05:    {h1a_criteria['significance']} (observed: {p_value:.4f})\"\n",
    ")\n",
    "print(\n",
    "    f\"  3. Effect size d≥0.5:      {h1a_criteria['effect_size']} (observed: {cohens_d:.3f})\"\n",
    ")\n",
    "print(f\"\\nOverall H1a Status: {h1a_criteria['overall_success']}\")\n",
    "\n",
    "if not h1a_criteria[\"overall_success\"]:\n",
    "    print(\"\\n❌ H1a NOT SUPPORTED\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"  - Collaborative mode UNDERPERFORMED rule-based baseline\")\n",
    "    print(\"  - No statistical evidence of benefit from feedback-guided mutations\")\n",
    "    print(\"  - Medium negative effect size suggests meaningful underperformance\")\n",
    "    print(\n",
    "        \"  - Root cause: Phase 1 MVP lacks feedback integration (planned for Phase 2)\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n✓ H1a SUPPORTED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Additional Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run-to-run variability\n",
    "print(\"Run-to-run Variability:\")\n",
    "print(\n",
    "    f\"  Collaborative CV: {(collaborative.std(ddof=1) / collaborative.mean()) * 100:.1f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"  Search-only CV:   {(search_only.std(ddof=1) / search_only.mean()) * 100:.1f}%\"\n",
    ")\n",
    "print(f\"  Rule-based CV:    {(rulebased.std(ddof=1) / rulebased.mean()) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if (search_only.std(ddof=1) / search_only.mean()) > 0.10:\n",
    "    print(\"  Search-only shows HIGH variability (CV>10%) - pure random search\")\n",
    "if (rulebased.std(ddof=1) / rulebased.mean()) < 0.10:\n",
    "    print(\n",
    "        \"  Rule-based shows MODERATE variability (CV<10%) - selection pressure reduces variance\"\n",
    "    )\n",
    "if (collaborative.std(ddof=1) / collaborative.mean()) > (\n",
    "    rulebased.std(ddof=1) / rulebased.mean()\n",
    "):\n",
    "    print(\n",
    "        \"  Collaborative MORE variable than rule-based - suggests lack of convergence\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise comparisons\n",
    "print(\"\\nPairwise Comparisons (Welch's t-test):\")\n",
    "\n",
    "# Collaborative vs. Search-only\n",
    "t1, p1 = stats.ttest_ind(collaborative, search_only, equal_var=False)\n",
    "d1 = (collaborative.mean() - search_only.mean()) / np.sqrt(\n",
    "    (collaborative.std(ddof=1) ** 2 + search_only.std(ddof=1) ** 2) / 2\n",
    ")\n",
    "print(\"\\n  Collaborative vs. Search-only:\")\n",
    "print(f\"    t={t1:.3f}, p={p1:.4f}, d={d1:.3f}\")\n",
    "print(f\"    Significant: {p1 < 0.05}\")\n",
    "\n",
    "# Collaborative vs. Rule-based (already calculated)\n",
    "print(\"\\n  Collaborative vs. Rule-based:\")\n",
    "print(f\"    t={t_stat:.3f}, p={p_value:.4f}, d={cohens_d:.3f}\")\n",
    "print(f\"    Significant: {p_value < 0.05}\")\n",
    "\n",
    "# Search-only vs. Rule-based\n",
    "t2, p2 = stats.ttest_ind(search_only, rulebased, equal_var=False)\n",
    "d2 = (search_only.mean() - rulebased.mean()) / np.sqrt(\n",
    "    (search_only.std(ddof=1) ** 2 + rulebased.std(ddof=1) ** 2) / 2\n",
    ")\n",
    "print(\"\\n  Search-only vs. Rule-based:\")\n",
    "print(f\"    t={t2:.3f}, p={p2:.4f}, d={d2:.3f}\")\n",
    "print(f\"    Significant: {p2 < 0.05}\")\n",
    "\n",
    "# Bonferroni correction for multiple comparisons\n",
    "alpha_bonferroni = 0.05 / 3\n",
    "print(f\"\\n  Bonferroni-corrected α: {alpha_bonferroni:.4f}\")\n",
    "print(f\"    Collaborative vs. Search-only: {p1 < alpha_bonferroni}\")\n",
    "print(f\"    Collaborative vs. Rule-based: {p_value < alpha_bonferroni}\")\n",
    "print(f\"    Search-only vs. Rule-based: {p2 < alpha_bonferroni}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions\n",
    "\n",
    "**Summary of Findings**:\n",
    "\n",
    "1. **Hypothesis H1a NOT SUPPORTED**: Collaborative mode underperformed rule-based baseline by 4.6%\n",
    "\n",
    "2. **Statistical Evidence**: \n",
    "   - No significant difference (p=0.579 > 0.05)\n",
    "   - Medium negative effect size (d=-0.58)\n",
    "   - 95% CIs show minimal overlap (~9%)\n",
    "\n",
    "3. **Root Cause**: Phase 1 MVP validated infrastructure but deferred feedback integration to Phase 2\n",
    "\n",
    "4. **Practical Implications**:\n",
    "   - Rule-based evolution remains strongest approach\n",
    "   - Collaborative mode needs Phase 2 implementation\n",
    "   - Infrastructure works correctly (agents communicate, no crashes)\n",
    "\n",
    "5. **Next Steps**:\n",
    "   - Implement Phase 2 feedback integration\n",
    "   - Re-run C2 validation with LLM-guided mutations\n",
    "   - Consider alternative hypotheses (H2: explainability, H3: hybrid approaches)\n",
    "\n",
    "**Final Assessment**: Negative result is scientifically valuable and guides Phase 2 design."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
